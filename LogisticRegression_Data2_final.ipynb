{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ba88e38-7575-4e06-b6f1-1d0dbae4566b",
   "metadata": {},
   "source": [
    "## Fundamental Principles, Assumptions, and Equations Involved\n",
    "\n",
    "A neural network is a computational model inspired by the way biological neural networks in the human brain process information. It consists of interconnected nodes (neurons) organized in layers: an input layer, one or more hidden layers, and an output layer. The fundamental principles of a neural network involve the following components and steps:\n",
    "\n",
    "- *Neurons and Layers*:\n",
    "  - Each neuron receives one or more inputs, processes them, and passes the output to the next layer.\n",
    "  - The neurons are organized in layers: input layer, hidden layers, and output layer.\n",
    "  \n",
    "- *Weights and Biases*:\n",
    "  - Each connection between neurons has a weight that determines the strength and direction of the connection.\n",
    "  - Each neuron also has an associated bias that shifts the activation function.\n",
    "  \n",
    "- *Activation Function*:\n",
    "  - The activation function introduces non-linearity into the model. Common activation functions include the sigmoid, tanh, and ReLU (Rectified Linear Unit).\n",
    "  \n",
    "- *Forward Propagation*:\n",
    "  - In forward propagation, inputs are passed through the network layer by layer to generate the output.\n",
    "  \n",
    "- *Loss Function*:\n",
    "  - The loss function quantifies the difference between the predicted output and the true output. For binary classification, the cross-entropy loss function is commonly used.\n",
    "  \n",
    "- *Backpropagation and Gradient Descent*:\n",
    "  - Backpropagation calculates the gradient of the loss function with respect to each weight by the chain rule, layer by layer backward from the output layer to the input layer.\n",
    "  - Gradient descent updates the weights to minimize the loss function.\n",
    "\n",
    "## Mathematical Equations\n",
    "\n",
    "### Forward Propagation\n",
    "\n",
    "For a neural network with one hidden layer:\n",
    "\n",
    "$$\n",
    "\\mathbf{z}^{(1)} = \\mathbf{X} \\mathbf{W}^{(1)} + \\mathbf{b}^{(1)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{a}^{(1)} = \\sigma(\\mathbf{z}^{(1)})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{z}^{(2)} = \\mathbf{a}^{(1)} \\mathbf{W}^{(2)} + \\mathbf{b}^{(2)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{a}^{(2)} = \\sigma(\\mathbf{z}^{(2)})\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{X}$ is the input matrix.\n",
    "- $\\mathbf{W}^{(1)}$ and $\\mathbf{W}^{(2)}$ are weight matrices for the hidden and output layers, respectively.\n",
    "- $\\mathbf{b}^{(1)}$ and $\\mathbf{b}^{(2)}$ are bias vectors for the hidden and output layers, respectively.\n",
    "- $\\sigma$ is the activation function (e.g., sigmoid function).\n",
    "\n",
    "### Cross-Entropy Loss\n",
    "\n",
    "$$\n",
    "L(\\mathbf{y}, \\mathbf{\\hat{y}}) = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{y}$ is the true label.\n",
    "- $\\mathbf{\\hat{y}}$ is the predicted label.\n",
    "- $N$ is the number of samples.\n",
    "\n",
    "### Backpropagation\n",
    "\n",
    "$$\n",
    "\\delta^{(2)} = \\mathbf{a}^{(2)} - \\mathbf{y}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}^{(2)}} = \\mathbf{a}^{(1)T} \\delta^{(2)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{b}^{(2)}} = \\sum \\delta^{(2)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\delta^{(1)} = (\\delta^{(2)} \\mathbf{W}^{(2)}) \\sigma'(\\mathbf{z}^{(1)})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}^{(1)}} = \\mathbf{X}^T \\delta^{(1)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{b}^{(1)}} = \\sum \\delta^{(1)}\n",
    "$$\n",
    "\n",
    "## How the Model Learns from Data and Makes Predictions\n",
    "\n",
    "1. *Initialization*:\n",
    "   - Initialize weights and biases randomly or using a specific initialization method.\n",
    "\n",
    "2. *Forward Pass*:\n",
    "   - Pass input data through the network to get predictions.\n",
    "\n",
    "3. *Compute Loss*:\n",
    "   - Calculate the loss using the cross-entropy loss function.\n",
    "\n",
    "4. *Backward Pass*:\n",
    "   - Perform backpropagation to compute gradients of the loss with respect to weights and biases.\n",
    "\n",
    "5. *Update Weights*:\n",
    "   - Update weights and biases using gradient descent or other optimization algorithms.\n",
    "\n",
    "6. *Repeat*:\n",
    "   - Repeat steps 2-5 for a number of epochs or untilÂ convergence.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf082d6-6dd2-4685-91a8-c30b578c23de",
   "metadata": {},
   "source": [
    "Importing important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "6a9c2bdb-4dc8-4c54-99f6-6727807a8a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511c33b3-a69a-4442-9646-990d033357ef",
   "metadata": {},
   "source": [
    "Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "a4a7cfc7-06ce-4d17-a9c3-cfcb15efeb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=\"C:/Users/DELL/Downloads/data2_train.csv\"\n",
    "df=pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "d1ce8a2a-c52d-4774-9c64-8877526e8907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 3)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "affcd3ee-cc4e-4f7b-adf1-24a151ed283b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature_1</th>\n",
       "      <th>Feature_2</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.160646</td>\n",
       "      <td>88.799326</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31.149536</td>\n",
       "      <td>102.335826</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.103383</td>\n",
       "      <td>92.902908</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15.950445</td>\n",
       "      <td>77.412565</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35.856965</td>\n",
       "      <td>94.441550</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature_1   Feature_2  Target\n",
       "0   8.160646   88.799326       0\n",
       "1  31.149536  102.335826       0\n",
       "2  13.103383   92.902908       0\n",
       "3  15.950445   77.412565       0\n",
       "4  35.856965   94.441550       0"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "c60818b2-2644-4a83-8ec2-e8c43cb60555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target\n",
      "0    419\n",
      "1    381\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for class balance\n",
    "print(df['Target'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd289e98-951a-4b6d-bb8b-e9fd74b55684",
   "metadata": {},
   "source": [
    "Function to define sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "a8efd25f-d482-4d0d-a6eb-3efb124be8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611cb929-7327-41c7-83fe-f599db113fc7",
   "metadata": {},
   "source": [
    "Function to implement Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "a6e8862d-8f44-438f-9047-b35ef32be619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X, Y, lr, n_iters=10000):\n",
    "    n_samples, n_features=X.shape\n",
    "    weights=np.zeros(n_features)\n",
    "    bias=0\n",
    "\n",
    "    for _ in range(n_iters):\n",
    "        linear_pred=np.dot(X, weights) + bias\n",
    "        predictions=sigmoid(linear_pred)\n",
    "\n",
    "        dw= (1/n_samples) * np.dot(X.T, (predictions-Y))\n",
    "        db= (1/n_samples) * np.sum(predictions-Y)\n",
    "\n",
    "        weights = weights-lr*dw\n",
    "        bias = bias-lr*db\n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6453ff66-f325-4768-8915-3f1202be73ff",
   "metadata": {},
   "source": [
    "Function to predict the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "4e8c7ed9-0914-457f-abd4-6a876b9a217d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, weights, bias):\n",
    "    linear_pred=np.dot(X, weights) + bias\n",
    "    y_pred=sigmoid(linear_pred)\n",
    "    class_pred=[0 if y<=0.5 else 1 for y in y_pred]\n",
    "    return class_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "50380a2c-cde5-4599-81ee-8f79960ab17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Feature_1', 'Feature_2']].values\n",
    "Y = df[['Target']].values.flatten()\n",
    "X_train, X_test, Y_train, Y_test= train_test_split(X,Y, test_size=0.2, random_state=1234)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5341ed44-031d-4186-882f-aca9def57cbf",
   "metadata": {},
   "source": [
    "Standardize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "d421520c-ade6-4bdb-ae9d-70611bd13915",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d500ea15-8eaa-493b-86dd-ec76224aa33a",
   "metadata": {},
   "source": [
    "Tuning of hyperparameter (learning rate) and training the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "e8b05d03-41dd-47ba-b356-3757b6c200d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy when learning_rate=0.01 is: 0.975\n",
      "accuracy when learning_rate=0.03 is: 0.975\n",
      "accuracy when learning_rate=0.05 is: 0.9\n"
     ]
    }
   ],
   "source": [
    "learning_rate=[0.01,0.03,0.05]\n",
    "max_acc=0\n",
    "for lr in learning_rate:\n",
    "    weights, bias=fit(X_train, Y_train,lr)\n",
    "    y_pred=predict(X_test, weights, bias)\n",
    "    accuracy=np.sum(y_pred==Y_test)/len(Y_test)\n",
    "    if max_acc<accuracy:\n",
    "        max_acc=accuracy\n",
    "        final_lr=lr\n",
    "    print(f\"accuracy when learning_rate={lr} is: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5111920-6878-451e-9e26-b501aca0350f",
   "metadata": {},
   "source": [
    "Implementing the Logistic Regression on test data and train data and calculating accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "5da1da58-aa31-478c-b0a2-dfdaafe028fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.93\n"
     ]
    }
   ],
   "source": [
    "file_path=\"C:/Users/DELL/Downloads/data2_train.csv\"\n",
    "df=pd.read_csv(file_path)\n",
    "\n",
    "X_train = df[['Feature_1', 'Feature_2']].values\n",
    "Y_train = df[['Target']].values.flatten()\n",
    "\n",
    "file_path=\"C:/Users/DELL/Downloads/data2_test.csv\"\n",
    "df_test=pd.read_csv(file_path)\n",
    "\n",
    "X_test = df_test[['Feature_1', 'Feature_2']].values\n",
    "Y_test = df_test[['Target']].values.flatten()\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "weights, bias=fit(X_train, Y_train,final_lr)\n",
    "y_pred=predict(X_test, weights, bias)\n",
    "\n",
    "df2=pd.DataFrame(X_test, columns=['Feature_1', 'Feature_2'])\n",
    "df2['Target']=Y_test\n",
    "df2['Predicted']=y_pred\n",
    "\n",
    "accuracy=np.sum(y_pred==Y_test)/len(Y_test)\n",
    "print (accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f61d328-34e4-4914-a82a-67137058ef02",
   "metadata": {},
   "source": [
    "Implementing the Logistic Regression using scikit learn on test data and train data and calculating accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "39aefa58-e6f7-4f8e-9719-117a70b43d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.93\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=10000)\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy=np.sum(y_pred==Y_test)/len(Y_test)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbfee77-9abe-4f05-8377-a861141a1099",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
