{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1ae94a7-5b29-423c-b298-b652d1da623c",
   "metadata": {},
   "source": [
    "## Fundamental Principles, Assumptions, and Equations Involved\n",
    "\n",
    "A neural network is a computational model inspired by the way biological neural networks in the human brain process information. It consists of interconnected nodes (neurons) organized in layers: an input layer, one or more hidden layers, and an output layer. The fundamental principles of a neural network involve the following components and steps:\n",
    "\n",
    "- *Neurons and Layers*:\n",
    "  - Each neuron receives one or more inputs, processes them, and passes the output to the next layer.\n",
    "  - The neurons are organized in layers: input layer, hidden layers, and output layer.\n",
    "  \n",
    "- *Weights and Biases*:\n",
    "  - Each connection between neurons has a weight that determines the strength and direction of the connection.\n",
    "  - Each neuron also has an associated bias that shifts the activation function.\n",
    "  \n",
    "- *Activation Function*:\n",
    "  - The activation function introduces non-linearity into the model. Common activation functions include the sigmoid, tanh, and ReLU (Rectified Linear Unit).\n",
    "  \n",
    "- *Forward Propagation*:\n",
    "  - In forward propagation, inputs are passed through the network layer by layer to generate the output.\n",
    "  \n",
    "- *Loss Function*:\n",
    "  - The loss function quantifies the difference between the predicted output and the true output. For binary classification, the cross-entropy loss function is commonly used.\n",
    "  \n",
    "- *Backpropagation and Gradient Descent*:\n",
    "  - Backpropagation calculates the gradient of the loss function with respect to each weight by the chain rule, layer by layer backward from the output layer to the input layer.\n",
    "  - Gradient descent updates the weights to minimize the loss function.\n",
    "\n",
    "## Mathematical Equations\n",
    "\n",
    "### Forward Propagation\n",
    "\n",
    "For a neural network with one hidden layer:\n",
    "\n",
    "$$\n",
    "\\mathbf{z}^{(1)} = \\mathbf{X} \\mathbf{W}^{(1)} + \\mathbf{b}^{(1)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{a}^{(1)} = \\sigma(\\mathbf{z}^{(1)})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{z}^{(2)} = \\mathbf{a}^{(1)} \\mathbf{W}^{(2)} + \\mathbf{b}^{(2)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{a}^{(2)} = \\sigma(\\mathbf{z}^{(2)})\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{X}$ is the input matrix.\n",
    "- $\\mathbf{W}^{(1)}$ and $\\mathbf{W}^{(2)}$ are weight matrices for the hidden and output layers, respectively.\n",
    "- $\\mathbf{b}^{(1)}$ and $\\mathbf{b}^{(2)}$ are bias vectors for the hidden and output layers, respectively.\n",
    "- $\\sigma$ is the activation function (e.g., sigmoid function).\n",
    "\n",
    "### Cross-Entropy Loss\n",
    "\n",
    "$$\n",
    "L(\\mathbf{y}, \\mathbf{\\hat{y}}) = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{y}$ is the true label.\n",
    "- $\\mathbf{\\hat{y}}$ is the predicted label.\n",
    "- $N$ is the number of samples.\n",
    "\n",
    "### Backpropagation\n",
    "\n",
    "$$\n",
    "\\delta^{(2)} = \\mathbf{a}^{(2)} - \\mathbf{y}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}^{(2)}} = \\mathbf{a}^{(1)T} \\delta^{(2)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{b}^{(2)}} = \\sum \\delta^{(2)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\delta^{(1)} = (\\delta^{(2)} \\mathbf{W}^{(2)}) \\sigma'(\\mathbf{z}^{(1)})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}^{(1)}} = \\mathbf{X}^T \\delta^{(1)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{b}^{(1)}} = \\sum \\delta^{(1)}\n",
    "$$\n",
    "\n",
    "## How the Model Learns from Data and Makes Predictions\n",
    "\n",
    "1. *Initialization*:\n",
    "   - Initialize weights and biases randomly or using a specific initialization method.\n",
    "\n",
    "2. *Forward Pass*:\n",
    "   - Pass input data through the network to get predictions.\n",
    "\n",
    "3. *Compute Loss*:\n",
    "   - Calculate the loss using the cross-entropy loss function.\n",
    "\n",
    "4. *Backward Pass*:\n",
    "   - Perform backpropagation to compute gradients of the loss with respect to weights and biases.\n",
    "\n",
    "5. *Update Weights*:\n",
    "   - Update weights and biases using gradient descent or other optimization algorithms.\n",
    "\n",
    "6. *Repeat*:\n",
    "   - Repeat steps 2-5 for a number of epochs or untilÂ convergence.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c78f3e3-6ae9-461f-b174-1e35c723ec58",
   "metadata": {},
   "source": [
    "Importing important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "922caaea-88db-443d-984b-a152399a58a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da22a00f-952b-4583-a525-ee85bd876c78",
   "metadata": {},
   "source": [
    "Functions to calculate sigmoid and its derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5043cc7f-181c-4d74-8852-5789aec282e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbdc0a8-5293-4cfd-a4c8-fc4989dedc83",
   "metadata": {},
   "source": [
    "Function to initialise the weights and bias with random values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "178b578e-a99b-4f1e-99d4-caa8de9554fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(input_dim, hidden_dim, output_dim):\n",
    "    np.random.seed(42)\n",
    "    W1 = np.random.randn(hidden_dim, input_dim) * 0.01\n",
    "    b1 = np.zeros((hidden_dim, 1))\n",
    "    W2 = np.random.randn(output_dim, hidden_dim) * 0.01\n",
    "    b2 = np.zeros((output_dim, 1))\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915d98c5-5da0-49cd-b3f5-7a518a226f1c",
   "metadata": {},
   "source": [
    "Function for forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d9932fd0-6699-443e-95fb-11feee8e28b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, W1, b1, W2, b2):\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    return Z1, A1, Z2, A2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25d2927-b29b-48e8-992f-08eb27f5d4ee",
   "metadata": {},
   "source": [
    "Function to compute the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "30f562e6-4288-4866-a764-93b95938047a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(A2, Y):\n",
    "    m = Y.shape[1]\n",
    "    # Ensure Y has the same shape as A2 for element-wise multiplication\n",
    "    Y = Y.reshape(A2.shape)  # reshape Y to match the shape of A2\n",
    "    logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1 - A2), 1 - Y)\n",
    "    loss = -np.sum(logprobs) / m\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2469a622-cbd5-4650-aabd-5776753da359",
   "metadata": {},
   "source": [
    "Function for backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e6fa0544-66f5-43fb-aec6-24a9d91ceee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(X, Y, Z1, A1, Z2, A2, W2):\n",
    "    m = X.shape[1]\n",
    "    Y = Y.reshape(A2.shape) \n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = np.dot(dZ2, A1.T) / m\n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True) / m\n",
    "    dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2))\n",
    "    dW1 = np.dot(dZ1, X.T) / m\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims=True) / m\n",
    "    return dW1, db1, dW2, db2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd9a3c7-432e-4dc2-88a1-6463e9242cda",
   "metadata": {},
   "source": [
    "Function to update the weights and bias to minimise loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "57fb4973-bf58-4398-af20-0dbcf4747a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95af64a9-af18-42e1-bd9f-ba04da542b04",
   "metadata": {},
   "source": [
    "Reading the csv file into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ea83f067-06b9-4862-842b-d8d75ef0c27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=\"C:/Users/DELL/Downloads/data2_train.csv\"\n",
    "df=pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "71a7cf3b-d843-4e26-ac2a-48b992d29c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Feature_1', 'Feature_2']].values\n",
    "y = df[['Target']].values.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084c8f12-b44a-4e70-92b0-e4893f380900",
   "metadata": {},
   "source": [
    "Split the dataset into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3b733d24-ea23-488e-af3e-1321ee4b611b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y.T, test_size=0.3, random_state=42)\n",
    "X_train, X_test = X_train.T, X_test.T  # Transpose to be (features, samples)\n",
    "y_train, y_test = y_train.T, y_test.T  # Transpose to be (1, samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccd186b-e316-4e2e-a3ba-14de010ad5d6",
   "metadata": {},
   "source": [
    "Standardize the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a137b409-3d38-4212-af87-97fe02c4a517",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.T).T  \n",
    "X_test = scaler.transform(X_test.T).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420ba82a-2286-4dc6-b3e6-1e22b4ad7d58",
   "metadata": {},
   "source": [
    "Initialize network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2836e6cb-210a-4bce-b82b-81b4cb224169",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[0]\n",
    "hidden_dim = 10\n",
    "output_dim = 1\n",
    "W1, b1, W2, b2 = initialize_parameters(input_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1ed6ea65-2764-4258-a9fe-73e617915e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = [0.01,0.05,0.005]\n",
    "no_iters = [10000, 20000, 30000]\n",
    "max_acc=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2461829a-4aaa-4bca-b413-72c58a33dbbf",
   "metadata": {},
   "source": [
    "Tuning of the hyperparameters (learning rate and number of iterations) and calculating accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "29ce14a8-3874-427d-9850-b6127b43ce18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for learning rate=0.01 and no. of iterations=10000: 99.17%\n",
      "Accuracy for learning rate=0.01 and no. of iterations=20000: 99.17%\n",
      "Accuracy for learning rate=0.01 and no. of iterations=30000: 99.17%\n",
      "Accuracy for learning rate=0.05 and no. of iterations=10000: 99.17%\n",
      "Accuracy for learning rate=0.05 and no. of iterations=20000: 99.17%\n",
      "Accuracy for learning rate=0.05 and no. of iterations=30000: 99.17%\n",
      "Accuracy for learning rate=0.005 and no. of iterations=10000: 99.17%\n",
      "Accuracy for learning rate=0.005 and no. of iterations=20000: 99.17%\n",
      "Accuracy for learning rate=0.005 and no. of iterations=30000: 99.17%\n"
     ]
    }
   ],
   "source": [
    "for learning_rate in lr:\n",
    "    for num_iterations in no_iters:\n",
    "        for i in range(num_iterations):\n",
    "            Z1, A1, Z2, A2 = forward_propagation(X_train, W1, b1, W2, b2)\n",
    "            loss = compute_loss(A2, y_train)\n",
    "            dW1, db1, dW2, db2 = backward_propagation(X_train, y_train, Z1, A1, Z2, A2, W2)\n",
    "            W1, b1, W2, b2 = update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n",
    "        \n",
    "            #if i % 1000 == 0:\n",
    "               # print(f'Iteration {i}, Loss: {loss:.4f}')\n",
    "        # Predict on test set\n",
    "        _, _, _, A2_test = forward_propagation(X_test, W1, b1, W2, b2)\n",
    "        predictions = (A2_test > 0.5).astype(int)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = np.mean(predictions == y_test)\n",
    "        if accuracy>max_acc:\n",
    "            max_acc=accuracy\n",
    "            final_lr=learning_rate\n",
    "            final_no_iters=num_iterations\n",
    "        print(f'Accuracy for learning rate={learning_rate} and no. of iterations={num_iterations}: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04aa3936-0baf-437a-a428-a1d843913665",
   "metadata": {},
   "source": [
    "Implementing the neural networks on training and testing data and calculating accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2b2f3f61-b344-4a7a-ad21-4e42266434c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Neural Network Accuracy: 99.00%\n"
     ]
    }
   ],
   "source": [
    "file_path=\"C:/Users/DELL/Downloads/data2_train.csv\"\n",
    "df=pd.read_csv(file_path)\n",
    "X_train = df[['Feature_1', 'Feature_2']].values.T\n",
    "y_train = df[['Target']].values.T\n",
    "\n",
    "file_path=\"C:/Users/DELL/Downloads/data2_test.csv\"\n",
    "df=pd.read_csv(file_path)\n",
    "X_test = df[['Feature_1', 'Feature_2']].values.T\n",
    "y_test = df[['Target']].values.T\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.T).T# Standardize and transpose back\n",
    "X_test = scaler.transform(X_test.T).T\n",
    "\n",
    "# Initialize network parameters\n",
    "input_dim = X_train.shape[0]\n",
    "hidden_dim = 10\n",
    "output_dim = 1\n",
    "W1, b1, W2, b2 = initialize_parameters(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "for i in range(final_no_iters):\n",
    "    Z1, A1, Z2, A2 = forward_propagation(X_train, W1, b1, W2, b2)\n",
    "    loss = compute_loss(A2, y_train)\n",
    "    dW1, db1, dW2, db2 = backward_propagation(X_train, y_train, Z1, A1, Z2, A2, W2)\n",
    "    W1, b1, W2, b2 = update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, final_lr)\n",
    "\n",
    "    #if i % 1000 == 0:\n",
    "     #   print(f'Iteration {i}, Loss: {loss:.4f}')\n",
    "\n",
    "# Predict on test set\n",
    "_, _, _, A2_test = forward_propagation(X_test, W1, b1, W2, b2)\n",
    "predictions = (A2_test > 0.5).astype(int)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f'Custom Neural Network Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9f8a99-5730-4631-b5f4-a8b6d11650fa",
   "metadata": {},
   "source": [
    "Implementing the neural networks from scikit learn on training and testing data and calculating accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2b08d3aa-c151-4110-a2e9-b1e10c91336a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit-learn MLPClassifier Accuracy: 99.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train=X_train.T\n",
    "X_test=X_test.T\n",
    "y_train=y_train.T.ravel()\n",
    "y_test=y_test.T.ravel()\n",
    "\n",
    "\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "# Initialize and train the MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10,), max_iter=10000, random_state=42)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "mlp_predictions = mlp.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "mlp_accuracy = accuracy_score(y_test, mlp_predictions)\n",
    "print(f'Scikit-learn MLPClassifier Accuracy: {mlp_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8965148-82e8-466a-ac01-185a4e515c64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
