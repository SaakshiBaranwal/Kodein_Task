{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0056f14c-ca50-4c12-ae8f-cb54446867a9",
   "metadata": {},
   "source": [
    "## Fundamental Principles, Assumptions, and Equations Involved\n",
    "\n",
    "A neural network is a computational model inspired by the way biological neural networks in the human brain process information. It consists of interconnected nodes (neurons) organized in layers: an input layer, one or more hidden layers, and an output layer. The fundamental principles of a neural network involve the following components and steps:\n",
    "\n",
    "- *Neurons and Layers*:\n",
    "  - Each neuron receives one or more inputs, processes them, and passes the output to the next layer.\n",
    "  - The neurons are organized in layers: input layer, hidden layers, and output layer.\n",
    "  \n",
    "- *Weights and Biases*:\n",
    "  - Each connection between neurons has a weight that determines the strength and direction of the connection.\n",
    "  - Each neuron also has an associated bias that shifts the activation function.\n",
    "  \n",
    "- *Activation Function*:\n",
    "  - The activation function introduces non-linearity into the model. Common activation functions include the sigmoid, tanh, and ReLU (Rectified Linear Unit).\n",
    "  \n",
    "- *Forward Propagation*:\n",
    "  - In forward propagation, inputs are passed through the network layer by layer to generate the output.\n",
    "  \n",
    "- *Loss Function*:\n",
    "  - The loss function quantifies the difference between the predicted output and the true output. For binary classification, the cross-entropy loss function is commonly used.\n",
    "  \n",
    "- *Backpropagation and Gradient Descent*:\n",
    "  - Backpropagation calculates the gradient of the loss function with respect to each weight by the chain rule, layer by layer backward from the output layer to the input layer.\n",
    "  - Gradient descent updates the weights to minimize the loss function.\n",
    "\n",
    "## Mathematical Equations\n",
    "\n",
    "### Forward Propagation\n",
    "\n",
    "For a neural network with one hidden layer:\n",
    "\n",
    "$$\n",
    "\\mathbf{z}^{(1)} = \\mathbf{X} \\mathbf{W}^{(1)} + \\mathbf{b}^{(1)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{a}^{(1)} = \\sigma(\\mathbf{z}^{(1)})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{z}^{(2)} = \\mathbf{a}^{(1)} \\mathbf{W}^{(2)} + \\mathbf{b}^{(2)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{a}^{(2)} = \\sigma(\\mathbf{z}^{(2)})\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{X}$ is the input matrix.\n",
    "- $\\mathbf{W}^{(1)}$ and $\\mathbf{W}^{(2)}$ are weight matrices for the hidden and output layers, respectively.\n",
    "- $\\mathbf{b}^{(1)}$ and $\\mathbf{b}^{(2)}$ are bias vectors for the hidden and output layers, respectively.\n",
    "- $\\sigma$ is the activation function (e.g., sigmoid function).\n",
    "\n",
    "### Cross-Entropy Loss\n",
    "\n",
    "$$\n",
    "L(\\mathbf{y}, \\mathbf{\\hat{y}}) = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{y}$ is the true label.\n",
    "- $\\mathbf{\\hat{y}}$ is the predicted label.\n",
    "- $N$ is the number of samples.\n",
    "\n",
    "### Backpropagation\n",
    "\n",
    "$$\n",
    "\\delta^{(2)} = \\mathbf{a}^{(2)} - \\mathbf{y}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}^{(2)}} = \\mathbf{a}^{(1)T} \\delta^{(2)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{b}^{(2)}} = \\sum \\delta^{(2)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\delta^{(1)} = (\\delta^{(2)} \\mathbf{W}^{(2)}) \\sigma'(\\mathbf{z}^{(1)})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}^{(1)}} = \\mathbf{X}^T \\delta^{(1)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{b}^{(1)}} = \\sum \\delta^{(1)}\n",
    "$$\n",
    "\n",
    "## How the Model Learns from Data and Makes Predictions\n",
    "\n",
    "1. *Initialization*:\n",
    "   - Initialize weights and biases randomly or using a specific initialization method.\n",
    "\n",
    "2. *Forward Pass*:\n",
    "   - Pass input data through the network to get predictions.\n",
    "\n",
    "3. *Compute Loss*:\n",
    "   - Calculate the loss using the cross-entropy loss function.\n",
    "\n",
    "4. *Backward Pass*:\n",
    "   - Perform backpropagation to compute gradients of the loss with respect to weights and biases.\n",
    "\n",
    "5. *Update Weights*:\n",
    "   - Update weights and biases using gradient descent or other optimization algorithms.\n",
    "\n",
    "6. *Repeat*:\n",
    "   - Repeat steps 2-5 for a number of epochs or untilÂ convergence.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1ddb06-fadf-4736-9d55-7ba4b43d6165",
   "metadata": {},
   "source": [
    "Importing important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "02079541-792e-488f-abd2-207f360d2657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4920248-66df-40f9-903d-5f7b31c14cb0",
   "metadata": {},
   "source": [
    "Reading and processing the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "614a3ece-562a-4816-bbb1-adb34660d7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = \"C:/Users/DELL/Downloads/data1_train.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Extract features and target\n",
    "X = df[['Feature_1', 'Feature_2', 'Feature_3']].values\n",
    "y = df['Target'].values\n",
    "\n",
    "# One-hot encode the target\n",
    "y_onehot = np.zeros((y.size, y.max() + 1))\n",
    "y_onehot[np.arange(y.size), y] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "47b5df40-0389-445c-a27c-5bb21f10f1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=1234)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d76d7d-7285-4b96-b431-1b7abeab5808",
   "metadata": {},
   "source": [
    "Softmax Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "30999295-b599-4bc6-a626-e52a49c1342e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax function\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8129bf29-9f92-47ec-a379-df292baf59ab",
   "metadata": {},
   "source": [
    "Function to implement Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d0c8d1c5-4a0d-468a-8dc3-2aa7c8172d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training function using gradient descent\n",
    "def fit(X_train, y_train, learning_rate, n_iters):\n",
    "    n_samples, n_features = X_train.shape\n",
    "    n_classes = y_train.shape[1]\n",
    "    \n",
    "    weights = np.zeros((n_features, n_classes))\n",
    "    bias = np.zeros(n_classes)\n",
    "    \n",
    "    for i in range(n_iters):\n",
    "        # Compute linear model\n",
    "        linear_model = np.dot(X_train, weights) + bias\n",
    "        # Apply softmax\n",
    "        y_pred = softmax(linear_model)\n",
    "        \n",
    "        # Compute gradients\n",
    "        dw = (1 / n_samples) * np.dot(X_train.T, (y_pred - y_train))\n",
    "        db = (1 / n_samples) * np.sum(y_pred - y_train, axis=0)\n",
    "        \n",
    "        # Update weights and bias\n",
    "        weights -= learning_rate * dw\n",
    "        bias -= learning_rate * db\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            # Compute loss (cross-entropy loss)\n",
    "            loss = -np.mean(np.sum(y_train * np.log(y_pred + 1e-15), axis=1))\n",
    "            print(f\"Iteration {i}: Loss = {loss}\")\n",
    "    \n",
    "    return weights, bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ead01c-ed2d-4fda-83f5-84a64607b4cc",
   "metadata": {},
   "source": [
    "Function to predict the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2f03d80a-b6e2-42b9-b3cf-94b8b7729600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction function\n",
    "def predict(X, weights, bias):\n",
    "    linear_model = np.dot(X, weights) + bias\n",
    "    y_pred = softmax(linear_model)\n",
    "    return np.argmax(y_pred, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673e5c68-4822-4e14-be02-880dbf4cf3a7",
   "metadata": {},
   "source": [
    "Tuning of hyperparameter (learning rate) and training the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6960fc6a-fa7b-4608-96f5-36d3bc4ee3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Loss = 1.098612288668107\n",
      "Iteration 1000: Loss = 0.20506980722622078\n",
      "Iteration 2000: Loss = 0.14928654862289742\n",
      "Iteration 3000: Loss = 0.12746718569356624\n",
      "Iteration 4000: Loss = 0.11544076298101484\n",
      "Iteration 5000: Loss = 0.10770540788906888\n",
      "Iteration 6000: Loss = 0.10226815873993218\n",
      "Iteration 7000: Loss = 0.0982185142200932\n",
      "Iteration 8000: Loss = 0.09507671734832063\n",
      "Iteration 9000: Loss = 0.09256411429074338\n",
      "Accuracy for learning rate=0.01% is: 98.12%\n",
      "Iteration 0: Loss = 1.098612288668107\n",
      "Iteration 1000: Loss = 0.12742374167283205\n",
      "Iteration 2000: Loss = 0.10225250695101529\n",
      "Iteration 3000: Loss = 0.09255567429721977\n",
      "Iteration 4000: Loss = 0.08733181453722975\n",
      "Iteration 5000: Loss = 0.0840585508737296\n",
      "Iteration 6000: Loss = 0.08182007580445042\n",
      "Iteration 7000: Loss = 0.08019895033066428\n",
      "Iteration 8000: Loss = 0.07897633157938785\n",
      "Iteration 9000: Loss = 0.07802596329653252\n",
      "Accuracy for learning rate=0.03% is: 98.12%\n",
      "Iteration 0: Loss = 1.098612288668107\n",
      "Iteration 1000: Loss = 0.10766430034490919\n",
      "Iteration 2000: Loss = 0.0904926143346871\n",
      "Iteration 3000: Loss = 0.08405481063821954\n",
      "Iteration 4000: Loss = 0.08068447096705109\n",
      "Iteration 5000: Loss = 0.07863232239380778\n",
      "Iteration 6000: Loss = 0.07726861283680839\n",
      "Iteration 7000: Loss = 0.07630896841517083\n",
      "Iteration 8000: Loss = 0.07560576481941728\n",
      "Iteration 9000: Loss = 0.07507470504754025\n",
      "Accuracy for learning rate=0.05% is: 98.12%\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "lr = [0.01,0.03,0.05]\n",
    "n_iters = 10000\n",
    "max_acc=0\n",
    "for learning_rate in lr:\n",
    "    weights, bias = fit(X_train, y_train, learning_rate, n_iters)\n",
    "    # Make predictions on test set\n",
    "    y_test_pred = predict(X_test, weights, bias)\n",
    "    y_test_actual = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(y_test_pred == y_test_actual) * 100\n",
    "    if max_acc<accuracy:\n",
    "        max_acc=accuracy\n",
    "        final_lr=learning_rate\n",
    "    print(f\"Accuracy for learning rate={learning_rate}% is: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef4487e-37e2-4e33-8e98-94e397e83e6f",
   "metadata": {},
   "source": [
    "Implementing the Multinomial Logistic Regression on test data and train data and calculating accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2e768b17-355a-40a7-833c-ff9631f06153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Loss = 1.098612288668107\n",
      "Iteration 1000: Loss = 0.19947257359384912\n",
      "Iteration 2000: Loss = 0.14323139422245404\n",
      "Iteration 3000: Loss = 0.12134719846601814\n",
      "Iteration 4000: Loss = 0.10932632763466288\n",
      "Iteration 5000: Loss = 0.1016084294723415\n",
      "Iteration 6000: Loss = 0.09618614796911679\n",
      "Iteration 7000: Loss = 0.09214558276939476\n",
      "Iteration 8000: Loss = 0.08900677098411297\n",
      "Iteration 9000: Loss = 0.08649177222575251\n",
      "Custom Logistic Regression Accuracy: 98.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assuming the fit, predict, and softmax functions are already defined as per previous discussions\n",
    "\n",
    "# Load training data\n",
    "file_path_train = \"C:/Users/DELL/Downloads/data1_train.csv\"\n",
    "df_train = pd.read_csv(file_path_train)\n",
    "\n",
    "X_train = df_train[['Feature_1', 'Feature_2', 'Feature_3']].values\n",
    "y_train = df_train['Target'].values\n",
    "\n",
    "# Load test data\n",
    "file_path_test = \"C:/Users/DELL/Downloads/data1_test.csv\"\n",
    "df_test = pd.read_csv(file_path_test)\n",
    "\n",
    "X_test = df_test[['Feature_1', 'Feature_2', 'Feature_3']].values\n",
    "y_test = df_test['Target'].values\n",
    "\n",
    "# One-hot encode the target (if necessary for your custom implementation)\n",
    "y_train_onehot = np.zeros((y_train.size, y_train.max() + 1))\n",
    "y_train_onehot[np.arange(y_train.size), y_train] = 1\n",
    "\n",
    "y_test_onehot = np.zeros((y_test.size, y_test.max() + 1))\n",
    "y_test_onehot[np.arange(y_test.size), y_test] = 1\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define and fit the model using custom implementation\n",
    "learning_rate = 0.01\n",
    "n_iters = 10000\n",
    "\n",
    "# Assuming the fit and predict functions are defined similarly as in the previous corrections\n",
    "\n",
    "# Fit the model\n",
    "weights, bias = fit(X_train, y_train_onehot, learning_rate, n_iters)\n",
    "\n",
    "# Make predictions on test set\n",
    "y_test_pred = predict(X_test, weights, bias)\n",
    "y_test_actual = np.argmax(y_test_onehot, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(y_test_pred == y_test_actual) * 100\n",
    "print(f\"Custom Logistic Regression Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24dcc75-b636-4283-a0f2-b5a7f9b5bb31",
   "metadata": {},
   "source": [
    "Implementing the Multinomial Logistic Regression using scikit learn on test data and train data and calculating accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "153e35d4-ddf8-44bb-b4ca-ed0495800641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn Logistic Regression Accuracy: 98.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda3\\envs\\Virtual_Anaconda_Env\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=10000)\n",
    "model.fit(X_train, y_train)\n",
    "sklearn_accuracy = model.score(X_test, y_test) * 100\n",
    "print(f\"Sklearn Logistic Regression Accuracy: {sklearn_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbe8dc0-f8bb-45ea-9346-e7d69635b065",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
